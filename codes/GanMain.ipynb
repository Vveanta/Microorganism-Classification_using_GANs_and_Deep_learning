{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CK4iuDQ0RHX"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBH5SsHf0dGR"
      },
      "source": [
        "#Define input image dimensions\n",
        "#Large images take too much time and resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCY-u6w10Zcu"
      },
      "outputs": [],
      "source": [
        "img_rows = 128\n",
        "img_cols = 128\n",
        "channels = 3\n",
        "img_shape = (img_rows, img_cols, channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JPjllIy5DRn"
      },
      "outputs": [],
      "source": [
        "!unzip Hydra.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoerFU9lnV36"
      },
      "outputs": [],
      "source": [
        "# !unzip /content/Common-Myna.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HskK6IzpSLyE"
      },
      "outputs": [],
      "source": [
        "# Create an ImageDataGenerator for data augmentation\n",
        "data_generator = ImageDataGenerator(\n",
        "    rotation_range=20,  # Random rotation up to 20 degrees\n",
        "    horizontal_flip=True,  # Random horizontal flip\n",
        "    vertical_flip=True,  # Random vertical flip\n",
        "    width_shift_range=0.1,  # Random horizontal shift\n",
        "    height_shift_range=0.1,  # Random vertical shift\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlY-XW7J5-f-",
        "outputId": "f098a206-d83a-4865-b976-72588fc696a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Hydra/Image_55.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_56.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_116.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_28.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_52.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_89.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_136.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_35.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_30.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_46.JPG  :  (128, 128, 3)\n",
            "/content/Hydra/Image_96.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_58.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_27.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_40.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_113.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_111.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_54.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_130.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_3.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_128.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_10.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_51.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_139.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_65.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_22.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_2.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_38.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_1.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_69.JPG  :  (128, 128, 3)\n",
            "/content/Hydra/Image_34.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_9.JPG  :  (128, 128, 3)\n",
            "/content/Hydra/Image_76.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_99.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_109.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_119.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_129.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_94.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_4.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_23.JPG  :  (128, 128, 3)\n",
            "/content/Hydra/Image_83.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_101.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_64.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_127.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_37.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_29.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_21.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_45.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_42.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_107.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_50.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_19.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_43.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_71.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_70.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_6.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_97.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_31.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_131.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_48.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_49.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_57.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_11.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_20.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_92.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_78.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_67.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_82.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_110.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_123.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_5.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_13.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_44.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_118.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_77.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_108.jpg  :  (128, 128, 3)\n",
            "/content/Hydra/Image_63.jpg  :  (128, 128, 3)\n"
          ]
        }
      ],
      "source": [
        "directory_path='/content/Hydra'\n",
        "# directory_path='/content/Common-Myna'\n",
        "\n",
        "file_paths=[]\n",
        "image_arrays=[]\n",
        "for filename in os.listdir(directory_path):\n",
        "  file_path = os.path.join(directory_path,filename)\n",
        "  file_paths.append(file_path)\n",
        "for path in file_paths:\n",
        "  image= Image.open(path)\n",
        "\n",
        "  image_resized = image.resize((img_rows, img_cols))\n",
        "  image_array= np.array(image_resized)\n",
        "  if image_array.shape!= 3:\n",
        "    image=image.convert('RGB')\n",
        "    image_resized = image.resize((img_rows, img_cols))\n",
        "    image_array= np.array(image_resized)\n",
        "  # Apply data augmentation\n",
        "    augmented_images = data_generator.flow(np.expand_dims(image_array, axis=0), batch_size=1)\n",
        "    augmented_image = next(augmented_images)[0]\n",
        "\n",
        "  print(path,' : ',image_array.shape)\n",
        "  image_arrays.append(image_array)\n",
        "  image_arrays.append(augmented_image)\n",
        "\n",
        "image_dataset= np.array(image_arrays, dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o34HCRNOAXnw"
      },
      "outputs": [],
      "source": [
        "for i in image_dataset:\n",
        "  if i.shape[-1]!= 3:\n",
        "    print(i.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk017CPvBNp1"
      },
      "outputs": [],
      "source": [
        "# Xtrain=image_dataset\n",
        "# print(Xtrain.shape)\n",
        "# t=mnist.load_data()[0][0]\n",
        "# print(t.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlcPlZWK0hZm"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "\n",
        "    noise_shape = (100,) #1D array of size 100 (latent vector / noise)\n",
        "\n",
        "#Define your generator network\n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(256, input_dim=100))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "    model.add(Reshape(img_shape))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    noise = Input(shape=noise_shape)\n",
        "    img = model(noise)    #Generated image\n",
        "\n",
        "    return Model(noise, img)\n",
        "\n",
        "#Alpha — α is a hyperparameter which controls the underlying value to which the\n",
        "#function saturates negatives network inputs.\n",
        "#Momentum — Speed up the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKz8U2wM0r3G"
      },
      "outputs": [],
      "source": [
        "def build_discriminator():\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Flatten(input_shape=img_shape))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "\n",
        "    img = Input(shape=img_shape)\n",
        "    validity = model(img)\n",
        "\n",
        "    return Model(img, validity)\n",
        "#The validity is the Discriminator’s guess of input being real or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu-GAle30zWz"
      },
      "source": [
        "#Now that we have constructed our two models it’s time to pit them against each other.\n",
        "#We do this by defining a training function, loading the data set, re-scaling our training\n",
        "#images and setting the ground truths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVragowu0w7R"
      },
      "outputs": [],
      "source": [
        "def train(epochs, batch_size=16, save_interval=50):\n",
        "\n",
        "    # Load the dataset\n",
        "    X_train= image_dataset\n",
        "\n",
        "    # Convert to float and Rescale -1 to 1 (Can also do 0 to 1)\n",
        "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "#Add channels dimension. As the input to our gen and discr. has a shape 28x28x1.\n",
        "    # X_train = np.expand_dims(X_train, axis=3)\n",
        "    half_batch = int(batch_size / 2)\n",
        "\n",
        "\n",
        "#We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "#a random batch of images from our true dataset, generating a set of images from our\n",
        "#Generator, feeding both set of images into our Discriminator, and finally setting the\n",
        "#loss parameters for both the real and fake images, as well as the combined loss.\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        # Select a random half batch of real images\n",
        "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "        imgs = X_train[idx]\n",
        "\n",
        "\n",
        "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
        "\n",
        "        # Generate a half batch of fake images\n",
        "        gen_imgs = generator.predict(noise)\n",
        "\n",
        "        # Train the discriminator on real and fake images, separately\n",
        "        #Research showed that separate training is more effective.\n",
        "        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
        "    #take average loss from real and fake images.\n",
        "    #\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "#And within the same loop we train our Generator, by setting the input noise and\n",
        "#ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "#by specifying the gradient loss.\n",
        "        # ---------------------\n",
        "        #  Train Generator\n",
        "        # ---------------------\n",
        "#Create noise vectors as input for generator.\n",
        "#Create as many noise vectors as defined by the batch size.\n",
        "#Based on normal distribution. Output will be of size (batch size, 100)\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "\n",
        "        # The generator wants the discriminator to label the generated samples\n",
        "        # as valid (ones)\n",
        "        #This is where the genrator is trying to trick discriminator into believing\n",
        "        #the generated image is true (hence value of 1 for y)\n",
        "        valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
        "\n",
        "        # Generator is part of combined where it got directly linked with the discriminator\n",
        "        # Train the generator with noise as x and 1 as y.\n",
        "        # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "        #job of folling the discriminator then the output would be 1 (true)\n",
        "        g_loss = combined.train_on_batch(noise, valid_y)\n",
        "\n",
        "\n",
        "#Additionally, in order for us to keep track of our training process, we print the\n",
        "#progress and save the sample image output depending on the epoch interval specified.\n",
        "# Plot the progress\n",
        "\n",
        "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "        # If at save interval => save generated image samples\n",
        "        if (epoch % save_interval == 0)or (d_loss[1]<0.6 and epoch > 10000):\n",
        "            save_imgs(epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWPQq07a08XU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def save_imgs(epoch):\n",
        "    r, c = 4, 4\n",
        "    noise = np.random.normal(0, 1, (r * c, 100))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "    # Create the 'images/' directory if it doesn't exist\n",
        "    os.makedirs('images_do/', exist_ok=True)\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            # axs[i,j].imshow(gen_imgs[cnt, :,:,0], )\n",
        "            axs[i,j].imshow(gen_imgs[cnt])\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"images_do/mnist_%d.png\" % epoch)\n",
        "    plt.close()\n",
        "#This function saves our images for us to view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65IO8Kq_1OIG"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcGz0hpJ1RRZ",
        "outputId": "7b1feed9-aaaa-4c57-8def-59d4050634dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 49152)             0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 512)               25166336  \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,297,921\n",
            "Trainable params: 25,297,921\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbTMspjd1WJB",
        "outputId": "eecc95c5-1892-4de8-c885-40ccf47875e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_10 (Dense)            (None, 256)               25856     \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 512)               131584    \n",
            "                                                                 \n",
            " leaky_re_lu_8 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 1024)              525312    \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 49152)             50380800  \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 128, 128, 3)       0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,070,720\n",
            "Trainable params: 51,067,136\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "generator = build_generator()\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is1LAzy61Z_4"
      },
      "outputs": [],
      "source": [
        "z = Input(shape=(100,))   #Our random input to the generator\n",
        "img = generator(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pcll_PSt1ct0"
      },
      "outputs": [],
      "source": [
        "discriminator.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_iTAAFu_mvP"
      },
      "outputs": [],
      "source": [
        "valid = discriminator(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAZTk1mf_njd"
      },
      "outputs": [],
      "source": [
        "combined = Model(z, valid)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fejr7b0f1iAS",
        "outputId": "a6d1e514-6aef-4140-ff91-63b976e96ef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 169ms/step\n",
            "0 [D loss: 0.951528, acc.: 43.75%] [G loss: 0.783294]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1 [D loss: 0.518696, acc.: 87.50%] [G loss: 0.659467]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "2 [D loss: 0.637408, acc.: 68.75%] [G loss: 0.541458]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "3 [D loss: 0.572582, acc.: 50.00%] [G loss: 0.516226]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "4 [D loss: 0.685313, acc.: 56.25%] [G loss: 0.251082]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "5 [D loss: 0.864772, acc.: 56.25%] [G loss: 0.494453]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "6 [D loss: 0.988689, acc.: 56.25%] [G loss: 0.232225]\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "7 [D loss: 1.180232, acc.: 50.00%] [G loss: 0.701150]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "8 [D loss: 1.155369, acc.: 56.25%] [G loss: 0.837875]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "9 [D loss: 1.563003, acc.: 62.50%] [G loss: 0.838647]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "10 [D loss: 1.080536, acc.: 68.75%] [G loss: 1.455802]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "11 [D loss: 3.366213, acc.: 50.00%] [G loss: 1.309214]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "12 [D loss: 1.542861, acc.: 62.50%] [G loss: 1.405508]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "13 [D loss: 1.459359, acc.: 68.75%] [G loss: 1.426340]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "14 [D loss: 1.404705, acc.: 62.50%] [G loss: 2.193683]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "15 [D loss: 0.947699, acc.: 75.00%] [G loss: 2.192341]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "16 [D loss: 5.121093, acc.: 56.25%] [G loss: 2.248047]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "17 [D loss: 3.921126, acc.: 75.00%] [G loss: 2.800644]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "18 [D loss: 9.357547, acc.: 43.75%] [G loss: 1.118583]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "19 [D loss: 3.145330, acc.: 68.75%] [G loss: 1.701659]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "20 [D loss: 4.705993, acc.: 18.75%] [G loss: 0.996917]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "21 [D loss: 6.355261, acc.: 68.75%] [G loss: 1.799248]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "22 [D loss: 4.907228, acc.: 68.75%] [G loss: 2.491446]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "23 [D loss: 2.481652, acc.: 62.50%] [G loss: 2.298367]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "24 [D loss: 5.061406, acc.: 56.25%] [G loss: 1.003945]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "25 [D loss: 2.719460, acc.: 50.00%] [G loss: 1.768080]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "26 [D loss: 3.884319, acc.: 50.00%] [G loss: 5.153943]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "27 [D loss: 4.459711, acc.: 68.75%] [G loss: 1.110508]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "28 [D loss: 2.570625, acc.: 56.25%] [G loss: 3.506803]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "29 [D loss: 11.327334, acc.: 25.00%] [G loss: 3.853878]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "30 [D loss: 4.252107, acc.: 50.00%] [G loss: 0.054363]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "31 [D loss: 2.133139, acc.: 75.00%] [G loss: 1.130288]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "32 [D loss: 3.581323, acc.: 68.75%] [G loss: 3.258542]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "33 [D loss: 1.043580, acc.: 81.25%] [G loss: 5.161416]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "34 [D loss: 3.679500, acc.: 56.25%] [G loss: 2.457350]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "35 [D loss: 12.221038, acc.: 25.00%] [G loss: 0.566184]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "36 [D loss: 14.191379, acc.: 62.50%] [G loss: 1.023858]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "37 [D loss: 6.586842, acc.: 68.75%] [G loss: 5.040825]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "38 [D loss: 0.297841, acc.: 81.25%] [G loss: 5.840443]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "39 [D loss: 1.151381, acc.: 81.25%] [G loss: 7.373246]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "40 [D loss: 12.230359, acc.: 31.25%] [G loss: 1.702980]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "41 [D loss: 9.079164, acc.: 50.00%] [G loss: 1.728922]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "42 [D loss: 5.372691, acc.: 62.50%] [G loss: 3.111240]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "43 [D loss: 7.777008, acc.: 43.75%] [G loss: 5.900631]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "44 [D loss: 0.900316, acc.: 87.50%] [G loss: 8.939678]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "45 [D loss: 9.474545, acc.: 31.25%] [G loss: 2.461022]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "46 [D loss: 6.139850, acc.: 62.50%] [G loss: 1.164366]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "47 [D loss: 3.936590, acc.: 75.00%] [G loss: 5.429032]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "48 [D loss: 4.236557, acc.: 62.50%] [G loss: 3.731524]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "49 [D loss: 5.333800, acc.: 62.50%] [G loss: 1.201828]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "50 [D loss: 5.256351, acc.: 56.25%] [G loss: 3.893681]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "51 [D loss: 1.143599, acc.: 81.25%] [G loss: 6.548341]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "52 [D loss: 8.653063, acc.: 25.00%] [G loss: 2.459968]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "53 [D loss: 5.546620, acc.: 56.25%] [G loss: 4.550584]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "54 [D loss: 13.827741, acc.: 18.75%] [G loss: 0.667512]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "55 [D loss: 3.542882, acc.: 56.25%] [G loss: 2.380452]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "56 [D loss: 15.000919, acc.: 25.00%] [G loss: 0.632740]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "57 [D loss: 3.006606, acc.: 68.75%] [G loss: 6.421592]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "58 [D loss: 3.184971, acc.: 62.50%] [G loss: 4.582494]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "59 [D loss: 1.874890, acc.: 56.25%] [G loss: 2.673174]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "60 [D loss: 0.062789, acc.: 100.00%] [G loss: 3.718401]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "61 [D loss: 1.746634, acc.: 87.50%] [G loss: 3.268619]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "62 [D loss: 6.079160, acc.: 50.00%] [G loss: 3.496958]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "63 [D loss: 2.156591, acc.: 75.00%] [G loss: 6.467310]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "64 [D loss: 8.590007, acc.: 37.50%] [G loss: 0.754275]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "65 [D loss: 4.035185, acc.: 62.50%] [G loss: 1.119829]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "66 [D loss: 4.010207, acc.: 62.50%] [G loss: 4.680956]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "67 [D loss: 1.067283, acc.: 68.75%] [G loss: 6.059944]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "68 [D loss: 4.266218, acc.: 50.00%] [G loss: 2.530726]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "69 [D loss: 4.594179, acc.: 50.00%] [G loss: 1.752206]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "70 [D loss: 1.849906, acc.: 68.75%] [G loss: 5.508094]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "71 [D loss: 1.369316, acc.: 81.25%] [G loss: 4.870838]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "72 [D loss: 0.925802, acc.: 75.00%] [G loss: 3.517804]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "73 [D loss: 1.463711, acc.: 68.75%] [G loss: 3.500546]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "74 [D loss: 7.828960, acc.: 43.75%] [G loss: 3.117003]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "75 [D loss: 9.530260, acc.: 31.25%] [G loss: 1.068048]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "76 [D loss: 3.926128, acc.: 62.50%] [G loss: 0.877006]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "77 [D loss: 1.130025, acc.: 75.00%] [G loss: 5.130350]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "78 [D loss: 5.652039, acc.: 43.75%] [G loss: 2.316943]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "79 [D loss: 7.941878, acc.: 56.25%] [G loss: 4.676868]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "80 [D loss: 6.849231, acc.: 37.50%] [G loss: 5.507129]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "81 [D loss: 0.144884, acc.: 93.75%] [G loss: 9.354027]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "82 [D loss: 2.629198, acc.: 50.00%] [G loss: 2.201058]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "83 [D loss: 3.565102, acc.: 68.75%] [G loss: 3.930574]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "84 [D loss: 2.291813, acc.: 68.75%] [G loss: 4.052103]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "85 [D loss: 9.899480, acc.: 31.25%] [G loss: 2.266246]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "86 [D loss: 8.579601, acc.: 18.75%] [G loss: 0.852819]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "87 [D loss: 2.846665, acc.: 81.25%] [G loss: 3.855530]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "88 [D loss: 3.549173, acc.: 68.75%] [G loss: 2.276294]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "89 [D loss: 0.981992, acc.: 81.25%] [G loss: 2.972933]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "90 [D loss: 4.163025, acc.: 62.50%] [G loss: 0.402077]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "91 [D loss: 2.463936, acc.: 62.50%] [G loss: 2.545511]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "92 [D loss: 3.641367, acc.: 62.50%] [G loss: 4.803171]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "93 [D loss: 0.705856, acc.: 81.25%] [G loss: 7.775585]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "94 [D loss: 6.062750, acc.: 25.00%] [G loss: 1.828283]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "95 [D loss: 3.469743, acc.: 62.50%] [G loss: 1.263491]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "96 [D loss: 2.700768, acc.: 56.25%] [G loss: 5.176275]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "97 [D loss: 9.193091, acc.: 31.25%] [G loss: 0.787117]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "98 [D loss: 4.344947, acc.: 50.00%] [G loss: 0.699154]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "99 [D loss: 2.217563, acc.: 68.75%] [G loss: 8.288457]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "100 [D loss: 5.494325, acc.: 56.25%] [G loss: 5.913969]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "101 [D loss: 6.234320, acc.: 37.50%] [G loss: 2.548089]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "102 [D loss: 6.680466, acc.: 68.75%] [G loss: 2.845968]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "103 [D loss: 6.430703, acc.: 50.00%] [G loss: 1.044978]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "104 [D loss: 0.593129, acc.: 81.25%] [G loss: 4.895120]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "105 [D loss: 4.189683, acc.: 43.75%] [G loss: 3.898319]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "106 [D loss: 3.246687, acc.: 68.75%] [G loss: 4.591516]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "107 [D loss: 4.414845, acc.: 37.50%] [G loss: 4.521107]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "108 [D loss: 11.742876, acc.: 12.50%] [G loss: 1.362447]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "109 [D loss: 9.322720, acc.: 68.75%] [G loss: 2.739424]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "110 [D loss: 8.646469, acc.: 50.00%] [G loss: 1.137216]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "111 [D loss: 0.916161, acc.: 93.75%] [G loss: 7.332318]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "112 [D loss: 16.095076, acc.: 18.75%] [G loss: 4.248962]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "113 [D loss: 6.143348, acc.: 50.00%] [G loss: 0.986840]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "114 [D loss: 3.379568, acc.: 68.75%] [G loss: 4.043851]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "115 [D loss: 0.308685, acc.: 87.50%] [G loss: 8.244822]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "116 [D loss: 10.951982, acc.: 25.00%] [G loss: 9.473482]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "117 [D loss: 12.195517, acc.: 81.25%] [G loss: 9.996961]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "118 [D loss: 27.454936, acc.: 25.00%] [G loss: 0.455229]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "119 [D loss: 5.368925, acc.: 62.50%] [G loss: 6.788256]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "120 [D loss: 0.713040, acc.: 93.75%] [G loss: 7.734134]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "121 [D loss: 2.379606, acc.: 68.75%] [G loss: 4.362033]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "122 [D loss: 2.843861, acc.: 56.25%] [G loss: 1.665040]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "123 [D loss: 1.144504, acc.: 75.00%] [G loss: 6.329434]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "124 [D loss: 7.339511, acc.: 31.25%] [G loss: 1.871286]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "125 [D loss: 2.584117, acc.: 87.50%] [G loss: 3.610596]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "126 [D loss: 8.078096, acc.: 56.25%] [G loss: 2.702750]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "127 [D loss: 6.579904, acc.: 62.50%] [G loss: 12.022832]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "128 [D loss: 2.484602, acc.: 75.00%] [G loss: 18.816082]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "129 [D loss: 2.050835, acc.: 68.75%] [G loss: 8.881470]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "130 [D loss: 7.769614, acc.: 37.50%] [G loss: 3.951301]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "131 [D loss: 1.188511, acc.: 75.00%] [G loss: 6.027033]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "132 [D loss: 5.489489, acc.: 43.75%] [G loss: 2.865475]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "133 [D loss: 4.671443, acc.: 75.00%] [G loss: 5.822343]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "134 [D loss: 3.086582, acc.: 56.25%] [G loss: 4.488281]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "135 [D loss: 0.637744, acc.: 75.00%] [G loss: 10.859060]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "136 [D loss: 6.069571, acc.: 43.75%] [G loss: 3.830299]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "137 [D loss: 1.503627, acc.: 75.00%] [G loss: 4.688734]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "138 [D loss: 2.310688, acc.: 68.75%] [G loss: 7.262246]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "139 [D loss: 1.219492, acc.: 75.00%] [G loss: 6.728541]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "140 [D loss: 7.097154, acc.: 37.50%] [G loss: 5.089185]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "141 [D loss: 4.626298, acc.: 68.75%] [G loss: 11.009672]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "142 [D loss: 2.729983, acc.: 75.00%] [G loss: 8.429114]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "143 [D loss: 4.809699, acc.: 43.75%] [G loss: 2.471262]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "144 [D loss: 7.157897, acc.: 62.50%] [G loss: 3.447488]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "145 [D loss: 3.065110, acc.: 56.25%] [G loss: 9.293785]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "146 [D loss: 1.314260, acc.: 50.00%] [G loss: 8.577262]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "147 [D loss: 8.682931, acc.: 43.75%] [G loss: 3.227527]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "148 [D loss: 1.451536, acc.: 75.00%] [G loss: 5.022200]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "149 [D loss: 1.117756, acc.: 75.00%] [G loss: 8.176863]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "150 [D loss: 4.384649, acc.: 31.25%] [G loss: 0.441025]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "151 [D loss: 4.519797, acc.: 62.50%] [G loss: 5.810414]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "152 [D loss: 0.243244, acc.: 81.25%] [G loss: 8.645809]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "153 [D loss: 1.461735, acc.: 62.50%] [G loss: 5.641222]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "154 [D loss: 2.356714, acc.: 75.00%] [G loss: 6.261786]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "155 [D loss: 6.436423, acc.: 25.00%] [G loss: 5.326651]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "156 [D loss: 7.769048, acc.: 50.00%] [G loss: 7.144321]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "157 [D loss: 1.384337, acc.: 75.00%] [G loss: 11.084049]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "158 [D loss: 4.895413, acc.: 87.50%] [G loss: 5.757081]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "159 [D loss: 2.284528, acc.: 56.25%] [G loss: 2.833212]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "160 [D loss: 4.066798, acc.: 68.75%] [G loss: 2.713836]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "161 [D loss: 0.618430, acc.: 87.50%] [G loss: 10.152475]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "162 [D loss: 2.252111, acc.: 68.75%] [G loss: 2.677033]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "163 [D loss: 0.570499, acc.: 93.75%] [G loss: 7.178810]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "164 [D loss: 0.858732, acc.: 87.50%] [G loss: 4.223774]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "165 [D loss: 2.383241, acc.: 81.25%] [G loss: 4.755137]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "166 [D loss: 1.651203, acc.: 75.00%] [G loss: 7.016103]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "167 [D loss: 0.558533, acc.: 81.25%] [G loss: 5.714652]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "168 [D loss: 2.755946, acc.: 25.00%] [G loss: 3.217334]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "169 [D loss: 4.561725, acc.: 62.50%] [G loss: 4.295239]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "170 [D loss: 9.068273, acc.: 31.25%] [G loss: 8.073789]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "171 [D loss: 1.922122, acc.: 68.75%] [G loss: 10.937605]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "172 [D loss: 4.198498, acc.: 43.75%] [G loss: 5.957236]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "173 [D loss: 2.236483, acc.: 62.50%] [G loss: 5.908815]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "174 [D loss: 4.915850, acc.: 50.00%] [G loss: 2.550563]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "175 [D loss: 4.241121, acc.: 56.25%] [G loss: 11.657369]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "176 [D loss: 4.399173, acc.: 56.25%] [G loss: 14.464497]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "177 [D loss: 11.265021, acc.: 12.50%] [G loss: 9.678196]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "178 [D loss: 2.245959, acc.: 56.25%] [G loss: 6.808714]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "179 [D loss: 1.224848, acc.: 75.00%] [G loss: 10.869405]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "180 [D loss: 3.165999, acc.: 43.75%] [G loss: 4.571691]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "181 [D loss: 0.943302, acc.: 62.50%] [G loss: 7.445747]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "182 [D loss: 2.061589, acc.: 68.75%] [G loss: 8.175841]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "183 [D loss: 3.372627, acc.: 50.00%] [G loss: 7.878462]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "184 [D loss: 1.678408, acc.: 81.25%] [G loss: 11.731470]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "185 [D loss: 3.695009, acc.: 56.25%] [G loss: 5.711432]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "186 [D loss: 1.830104, acc.: 62.50%] [G loss: 4.206334]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "187 [D loss: 4.016710, acc.: 68.75%] [G loss: 5.517825]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "188 [D loss: 1.965195, acc.: 62.50%] [G loss: 7.403168]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "189 [D loss: 3.642136, acc.: 62.50%] [G loss: 1.867084]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "190 [D loss: 4.439010, acc.: 56.25%] [G loss: 6.259303]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "191 [D loss: 1.496992, acc.: 62.50%] [G loss: 10.714609]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "192 [D loss: 1.552866, acc.: 81.25%] [G loss: 3.134672]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "193 [D loss: 2.215731, acc.: 68.75%] [G loss: 2.301975]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "194 [D loss: 1.483856, acc.: 62.50%] [G loss: 5.527230]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "195 [D loss: 2.906233, acc.: 50.00%] [G loss: 3.627115]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "196 [D loss: 5.669669, acc.: 62.50%] [G loss: 3.527590]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "197 [D loss: 2.606279, acc.: 50.00%] [G loss: 7.331802]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "198 [D loss: 8.644496, acc.: 43.75%] [G loss: 7.328092]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "199 [D loss: 1.809636, acc.: 81.25%] [G loss: 10.919551]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "200 [D loss: 3.981525, acc.: 68.75%] [G loss: 6.537209]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "201 [D loss: 1.343270, acc.: 62.50%] [G loss: 3.002632]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "202 [D loss: 0.109440, acc.: 93.75%] [G loss: 6.861750]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "203 [D loss: 1.270158, acc.: 56.25%] [G loss: 4.243969]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "204 [D loss: 1.174865, acc.: 68.75%] [G loss: 2.275454]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "205 [D loss: 1.108954, acc.: 81.25%] [G loss: 4.737384]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "206 [D loss: 1.898895, acc.: 68.75%] [G loss: 3.820601]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "207 [D loss: 2.298300, acc.: 68.75%] [G loss: 6.196515]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "208 [D loss: 3.701631, acc.: 50.00%] [G loss: 3.805811]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "209 [D loss: 4.337540, acc.: 50.00%] [G loss: 1.460447]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "210 [D loss: 2.436726, acc.: 75.00%] [G loss: 16.213394]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "211 [D loss: 10.497711, acc.: 31.25%] [G loss: 6.652829]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "212 [D loss: 1.273363, acc.: 56.25%] [G loss: 11.643654]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "213 [D loss: 7.926610, acc.: 43.75%] [G loss: 4.864662]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "214 [D loss: 1.613634, acc.: 62.50%] [G loss: 7.722320]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "215 [D loss: 4.062017, acc.: 31.25%] [G loss: 1.414271]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "216 [D loss: 0.351977, acc.: 81.25%] [G loss: 10.547621]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "217 [D loss: 7.983099, acc.: 25.00%] [G loss: 5.496136]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "218 [D loss: 7.871030, acc.: 43.75%] [G loss: 7.992188]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "219 [D loss: 1.118847, acc.: 81.25%] [G loss: 16.245647]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "220 [D loss: 9.177333, acc.: 62.50%] [G loss: 4.476215]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "221 [D loss: 0.085149, acc.: 100.00%] [G loss: 5.457522]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "222 [D loss: 2.043294, acc.: 75.00%] [G loss: 2.868193]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "223 [D loss: 2.390009, acc.: 56.25%] [G loss: 2.987894]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "224 [D loss: 5.943475, acc.: 43.75%] [G loss: 4.450022]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "225 [D loss: 2.923219, acc.: 37.50%] [G loss: 3.827016]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "226 [D loss: 5.066569, acc.: 56.25%] [G loss: 2.615807]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "227 [D loss: 1.214914, acc.: 81.25%] [G loss: 8.061388]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "228 [D loss: 3.948172, acc.: 50.00%] [G loss: 2.112188]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "229 [D loss: 0.579199, acc.: 81.25%] [G loss: 5.425489]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "230 [D loss: 1.106141, acc.: 62.50%] [G loss: 10.350219]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "231 [D loss: 4.665045, acc.: 62.50%] [G loss: 2.880504]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "232 [D loss: 1.450069, acc.: 62.50%] [G loss: 6.889328]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "233 [D loss: 1.853477, acc.: 87.50%] [G loss: 6.515483]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "234 [D loss: 3.520629, acc.: 62.50%] [G loss: 8.581357]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "235 [D loss: 4.238562, acc.: 31.25%] [G loss: 6.414794]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "236 [D loss: 2.287084, acc.: 87.50%] [G loss: 11.389111]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "237 [D loss: 5.601000, acc.: 56.25%] [G loss: 12.162405]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "238 [D loss: 4.593199, acc.: 43.75%] [G loss: 2.369451]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "239 [D loss: 3.066473, acc.: 56.25%] [G loss: 14.428577]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "240 [D loss: 5.356450, acc.: 62.50%] [G loss: 9.494354]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "241 [D loss: 6.701260, acc.: 37.50%] [G loss: 2.697069]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "242 [D loss: 0.642246, acc.: 81.25%] [G loss: 6.523515]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "243 [D loss: 3.162060, acc.: 50.00%] [G loss: 2.989616]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "244 [D loss: 1.402069, acc.: 62.50%] [G loss: 7.003294]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "245 [D loss: 1.562922, acc.: 75.00%] [G loss: 6.775506]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "246 [D loss: 3.568698, acc.: 31.25%] [G loss: 1.959932]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "247 [D loss: 1.825427, acc.: 81.25%] [G loss: 5.750638]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "248 [D loss: 4.060973, acc.: 50.00%] [G loss: 0.881854]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "249 [D loss: 1.337632, acc.: 75.00%] [G loss: 5.246839]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "250 [D loss: 0.663056, acc.: 81.25%] [G loss: 4.457928]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "251 [D loss: 5.601072, acc.: 37.50%] [G loss: 1.674272]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "252 [D loss: 1.704837, acc.: 68.75%] [G loss: 4.180644]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "253 [D loss: 2.254915, acc.: 43.75%] [G loss: 9.935875]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "254 [D loss: 4.183702, acc.: 50.00%] [G loss: 4.683232]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "255 [D loss: 3.469967, acc.: 62.50%] [G loss: 6.925199]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "256 [D loss: 2.255016, acc.: 37.50%] [G loss: 5.882051]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "257 [D loss: 0.582994, acc.: 81.25%] [G loss: 6.841387]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "258 [D loss: 1.197395, acc.: 68.75%] [G loss: 7.012452]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "259 [D loss: 0.984673, acc.: 75.00%] [G loss: 7.048179]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "260 [D loss: 1.577763, acc.: 62.50%] [G loss: 2.242800]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "261 [D loss: 0.329740, acc.: 75.00%] [G loss: 4.593857]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "262 [D loss: 1.357017, acc.: 75.00%] [G loss: 4.545363]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "263 [D loss: 4.096569, acc.: 50.00%] [G loss: 7.858594]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "264 [D loss: 2.362422, acc.: 62.50%] [G loss: 8.801645]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "265 [D loss: 3.199664, acc.: 50.00%] [G loss: 6.024471]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "266 [D loss: 0.648379, acc.: 81.25%] [G loss: 6.305655]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "267 [D loss: 3.486279, acc.: 43.75%] [G loss: 6.081308]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "268 [D loss: 4.157660, acc.: 56.25%] [G loss: 8.816260]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "269 [D loss: 2.309091, acc.: 68.75%] [G loss: 14.860860]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "270 [D loss: 6.563184, acc.: 31.25%] [G loss: 1.874478]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "271 [D loss: 5.993313, acc.: 56.25%] [G loss: 4.370400]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "272 [D loss: 3.458805, acc.: 75.00%] [G loss: 11.538567]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "273 [D loss: 5.176455, acc.: 43.75%] [G loss: 4.742925]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "274 [D loss: 2.325465, acc.: 81.25%] [G loss: 6.726174]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "275 [D loss: 0.073135, acc.: 93.75%] [G loss: 6.896023]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "276 [D loss: 3.638213, acc.: 62.50%] [G loss: 7.696137]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "277 [D loss: 4.591914, acc.: 56.25%] [G loss: 8.584864]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "278 [D loss: 2.394298, acc.: 81.25%] [G loss: 5.447055]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "279 [D loss: 7.712105, acc.: 50.00%] [G loss: 4.926998]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "280 [D loss: 4.000307, acc.: 56.25%] [G loss: 9.650515]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "281 [D loss: 3.254341, acc.: 43.75%] [G loss: 6.396461]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "282 [D loss: 1.367763, acc.: 81.25%] [G loss: 6.799105]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "283 [D loss: 2.975006, acc.: 56.25%] [G loss: 8.053251]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "284 [D loss: 0.453681, acc.: 81.25%] [G loss: 10.351051]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "285 [D loss: 6.927926, acc.: 37.50%] [G loss: 3.482749]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "286 [D loss: 2.034495, acc.: 62.50%] [G loss: 1.028911]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "287 [D loss: 2.840601, acc.: 62.50%] [G loss: 12.953962]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "288 [D loss: 2.613014, acc.: 56.25%] [G loss: 3.974751]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "289 [D loss: 3.663107, acc.: 62.50%] [G loss: 2.052463]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "290 [D loss: 1.818768, acc.: 75.00%] [G loss: 1.933225]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "291 [D loss: 0.769382, acc.: 87.50%] [G loss: 3.857235]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "292 [D loss: 7.545490, acc.: 31.25%] [G loss: 3.972074]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "293 [D loss: 2.434050, acc.: 56.25%] [G loss: 5.790695]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "294 [D loss: 2.894614, acc.: 43.75%] [G loss: 6.135016]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "295 [D loss: 3.498448, acc.: 50.00%] [G loss: 4.868375]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "296 [D loss: 4.285936, acc.: 37.50%] [G loss: 1.467863]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "297 [D loss: 0.916634, acc.: 87.50%] [G loss: 6.248610]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "298 [D loss: 14.187178, acc.: 12.50%] [G loss: 2.142342]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "299 [D loss: 1.957356, acc.: 56.25%] [G loss: 11.339653]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "300 [D loss: 5.084817, acc.: 62.50%] [G loss: 4.367414]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "301 [D loss: 2.767804, acc.: 56.25%] [G loss: 9.323378]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "302 [D loss: 4.013687, acc.: 43.75%] [G loss: 2.124830]\n",
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        }
      ],
      "source": [
        "train(epochs=30000, batch_size=16, save_interval=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator.save('hydra_generator_30kepochs.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBteOMqNOjBe"
      },
      "outputs": [],
      "source": [
        "# r, c = 5, 5\n",
        "# noise = np.random.normal(0, 1, (r * c, 100))\n",
        "# gen_imgs = generator.predict(noise)\n",
        "\n",
        "# # Rescale images 0 - 1\n",
        "# gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "# # Create the 'images_do/' directory if it doesn't exist\n",
        "# os.makedirs('images_dot/', exist_ok=True)\n",
        "\n",
        "# fig, axs = plt.subplots(r, c)\n",
        "# cnt = 0\n",
        "# for i in range(r):\n",
        "#     for j in range(c):\n",
        "#         axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "#         # axs[i, j].imshow(gen_imgs[cnt, :, :, :])\n",
        "#         axs[i, j].axis('off')\n",
        "#         cnt += 1\n",
        "\n",
        "# fig.savefig(\"images_dot/mnist_%d.png\")\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7PyoEPe05gq"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
